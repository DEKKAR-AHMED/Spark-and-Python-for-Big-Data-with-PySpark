{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Review "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Textuel Data (Text mining )\n",
    "- it very large field in machine learning \n",
    "- NLTK book (Seprate python Library )\n",
    "- Foundations of Statistical Natural Language Processing ( by Manning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP used For ?\n",
    "- Clustering New Articles \n",
    "- Seggestion similar books \n",
    "- Analysing Comnsumer FeedBack \n",
    "- NLP process :\n",
    "    - Corpus - Bag of word \n",
    "    - Featureise the words to numerics \n",
    "    - Compare feature of documents \n",
    "         - to compare this we gonna use TF- IDF we can also take a count of DR (AFC)\n",
    "         \n",
    "         \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"nlp\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lets talk about tokenized and tokenization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tokenization is a preocess of taking a text such as a sentence and then breaking it into list of individual terms\n",
    "- for most powerfull processing textual donne see (Regular expression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
    "from pyspark.sql.functions import col,udf\n",
    "# col allow us to call a columns and then use the fonction using lamba expression\n",
    "from pyspark.sql.types import IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create A data Frame\n",
    "send_df = spark.createDataFrame([\n",
    "    (0, \"hi i heard about Spark\"),\n",
    "    (1,\"I wish java could use case classes \"),\n",
    "    (2,\"Logistic,regression,models,are,neat\")\n",
    "],[\"id\",\"Sentences\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|           Sentences|\n",
      "+---+--------------------+\n",
      "|  0|hi i heard about ...|\n",
      "|  1|I wish java could...|\n",
      "|  2|Logistic,regressi...|\n",
      "+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "send_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = Tokenizer(inputCol=\"Sentences\",outputCol=\"words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex = RegexTokenizer(inputCol=\"Sentences\",outputCol=\"words\", pattern = \"\\\\W\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_token = udf(lambda words:len(words), IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+\n",
      "| id|           Sentences|               words|\n",
      "+---+--------------------+--------------------+\n",
      "|  0|hi i heard about ...|[hi, i, heard, ab...|\n",
      "|  1|I wish java could...|[i, wish, java, c...|\n",
      "|  2|Logistic,regressi...|[logistic,regress...|\n",
      "+---+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenized = token.transform(send_df)\n",
    "tokenized.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+------+\n",
      "| id|           Sentences|               words|tokens|\n",
      "+---+--------------------+--------------------+------+\n",
      "|  0|hi i heard about ...|[hi, i, heard, ab...|     5|\n",
      "|  1|I wish java could...|[i, wish, java, c...|     7|\n",
      "|  2|Logistic,regressi...|[logistic,regress...|     1|\n",
      "+---+--------------------+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenized.withColumn(\"tokens\",count_token(col(\"words\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+\n",
      "| id|           Sentences|               words|\n",
      "+---+--------------------+--------------------+\n",
      "|  0|hi i heard about ...|[hi, i, heard, ab...|\n",
      "|  1|I wish java could...|[i, wish, java, c...|\n",
      "|  2|Logistic,regressi...|[logistic, regres...|\n",
      "+---+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rg =regex.transform(send_df)\n",
    "rg.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+------+\n",
      "| id|           Sentences|               words|tokens|\n",
      "+---+--------------------+--------------------+------+\n",
      "|  0|hi i heard about ...|[hi, i, heard, ab...|     5|\n",
      "|  1|I wish java could...|[i, wish, java, c...|     7|\n",
      "|  2|Logistic,regressi...|[logistic, regres...|     5|\n",
      "+---+--------------------+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rg.withColumn(\"tokens\",count_token(col(\"words\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_dataframe = spark.createDataFrame([\n",
    "    (0,[\"i\",\"saw\",\"the\",\"green\",\"horse\"]),\n",
    "    (1,[\"mary\",\"had\",\"a\",\"little\",\"lamb\"]),\n",
    "    \n",
    "], [\"id\",\"tokens\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "remover = StopWordsRemover(inputCol=\"tokens\",outputCol=\"filtred\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+\n",
      "| id|              tokens|             filtred|\n",
      "+---+--------------------+--------------------+\n",
      "|  0|[i, saw, the, gre...| [saw, green, horse]|\n",
      "|  1|[mary, had, a, li...|[mary, little, lamb]|\n",
      "+---+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "remover.transform(sentence_dataframe).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# n-gram\n",
    "it s a sequence of n tokens for some integer N and the angram class can be used to transform input features to ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import NGram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create A data Frame\n",
    "send_df = spark.createDataFrame([\n",
    "    (0,[ \"hi\", \"i\" ,\"heard\", \"about\", \"Spark\"]),\n",
    "    (1,[\"I\", \"wish\", \"java\", \"could\", \"use\", \"case\", \"classes\"]),\n",
    "    (2,[\"Logistic\",\"regression\",\"models\",\"are\",\"neat\"])],[\"id\",\"words\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram = NGram(n = 3 , inputCol=\"words\", outputCol=\"grams\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------+\n",
      "|grams                                                                           |\n",
      "+--------------------------------------------------------------------------------+\n",
      "|[hi i heard, i heard about, heard about Spark]                                  |\n",
      "|[I wish java, wish java could, java could use, could use case, use case classes]|\n",
      "|[Logistic regression models, regression models are, models are neat]            |\n",
      "+--------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ngram.transform(send_df).select(\"grams\").show(truncate= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this kind of ngram is usefull when we want the relationships between two words, which words alwas appear next each other "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Term-fequency and Doc Fequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF,Tokenizer\n",
    "# Hashing TF for term Frequency , IDF for univer Document frequency, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create A data Frame\n",
    "send_df = spark.createDataFrame([\n",
    "    (0, \"hi i heard about Spark\"),\n",
    "    (1,\"I wish java could use case classes \"),\n",
    "    (2,\"Logistic,regression,models,are,neat\")\n",
    "],[\"id\",\"Sentences\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|           Sentences|\n",
      "+---+--------------------+\n",
      "|  0|hi i heard about ...|\n",
      "|  1|I wish java could...|\n",
      "|  2|Logistic,regressi...|\n",
      "+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "send_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = Tokenizer(inputCol=\"Sentences\",outputCol=\"words\")\n",
    "#regex = RegexTokenizer(inputCol=\"Sentences\",outputCol=\"words\", pattern = \"\\\\W\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_data = token.transform(send_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+\n",
      "| id|           Sentences|               words|\n",
      "+---+--------------------+--------------------+\n",
      "|  0|hi i heard about ...|[hi, i, heard, ab...|\n",
      "|  1|I wish java could...|[i, wish, java, c...|\n",
      "|  2|Logistic,regressi...|[logistic,regress...|\n",
      "+---+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "words_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashing_td = HashingTF(inputCol=\"words\",outputCol=\"rawFeatures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurized_data = hashing_td.transform(words_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = IDF(inputCol=\"rawFeatures\",outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf_model = idf.fit(featurized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "rescaled_data = idf_model.transform(featurized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|            features|\n",
      "+---+--------------------+\n",
      "|  0|(262144,[18700,19...|\n",
      "|  1|(262144,[19036,20...|\n",
      "|  2|(262144,[11534],[...|\n",
      "+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rescaled_data.select(\"id\",\"features\").show()\n",
    "# transformed the the textuel data to numeric "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count Vectorizer\n",
    "from pyspark.ml.feature import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([\n",
    "    \n",
    "    (0,\"a b c \".split(\" \")),\n",
    "      (0,\"a b b c a \".split(\" \"))], [ \"id\",\"words\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------+\n",
      "| id|            words|\n",
      "+---+-----------------+\n",
      "|  0|      [a, b, c, ]|\n",
      "|  0|[a, b, b, c, a, ]|\n",
      "+---+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the best words and or the most pertinent work that are accored in more than one doc \n",
    "cv = CountVectorizer(inputCol=\"words\",outputCol=\"features\", vocabSize= 3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = cv.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------+-------------------------+\n",
      "|id |words            |features                 |\n",
      "+---+-----------------+-------------------------+\n",
      "|0  |[a, b, c, ]      |(3,[0,1,2],[1.0,1.0,1.0])|\n",
      "|0  |[a, b, b, c, a, ]|(3,[0,1,2],[2.0,2.0,1.0])|\n",
      "+---+-----------------+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = model.transform(df).show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
