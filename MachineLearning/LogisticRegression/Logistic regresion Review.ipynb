{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regresion Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - binary classification (Classification algorithm)\n",
    " - for example : Spam/ email , Customers (laon default),...\n",
    " - predict two Classes 0 and 1 \n",
    " - we use \"Sigmoid\" function or logistic functions takes in any value and output it to be between 0 and 1 \n",
    " - The difference between Linear Regression and logistic Regression is that we can say we take our linear Regression Solution and we put it into sigmoid Funtion.\n",
    " - we can set a cutoff point at 0.5 anything below the result be 0 and anything above the result will be 1 \n",
    " - Evalution Logistic Regression (Matric Confusion)\n",
    " \n",
    " - accurary = somme (True positive +True Negative) / Total\n",
    " - Recall = True positive / (True Positive + Faux negative)\n",
    " - ... the confusion can lead us to all this metrics\n",
    " - the good metrics will really depends on the specific situation sometime we take the accuracy some time we \n",
    " take the recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  accuracy \n",
    "answer the question how often is it correct  if have a balanced dataset it s good \n",
    "### Error Rate (Miss classification Rate)\n",
    "how often  we are wrong "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Curve ROC \n",
    "is just a plot sensitivite (True Postive Rate) versus  - specificity (Fakse Positive Rate )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code an Example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"logReg_test\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data = spark.read.format(\"libsvm\").load(\"../../Data/sample_libsvm_data.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  0.0|(692,[127,128,129...|\n",
      "|  1.0|(692,[158,159,160...|\n",
      "|  1.0|(692,[124,125,126...|\n",
      "|  1.0|(692,[152,153,154...|\n",
      "|  1.0|(692,[151,152,153...|\n",
      "|  0.0|(692,[129,130,131...|\n",
      "|  1.0|(692,[158,159,160...|\n",
      "|  1.0|(692,[99,100,101,...|\n",
      "|  0.0|(692,[154,155,156...|\n",
      "|  0.0|(692,[127,128,129...|\n",
      "|  1.0|(692,[154,155,156...|\n",
      "|  0.0|(692,[153,154,155...|\n",
      "|  0.0|(692,[151,152,153...|\n",
      "|  1.0|(692,[129,130,131...|\n",
      "|  0.0|(692,[154,155,156...|\n",
      "|  1.0|(692,[150,151,152...|\n",
      "|  0.0|(692,[124,125,126...|\n",
      "|  0.0|(692,[152,153,154...|\n",
      "|  1.0|(692,[97,98,99,12...|\n",
      "|  1.0|(692,[124,125,126...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "my_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create our logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgr_model = lgr.fit(my_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_summary = lgr_model.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_summary.predictions.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|label|            features|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|  0.0|(692,[127,128,129...|[19.8534775947478...|[0.99999999761359...|       0.0|\n",
      "|  1.0|(692,[158,159,160...|[-20.377398194908...|[1.41321555111051...|       1.0|\n",
      "|  1.0|(692,[124,125,126...|[-27.401459284891...|[1.25804865126979...|       1.0|\n",
      "|  1.0|(692,[152,153,154...|[-18.862741612668...|[6.42710509170301...|       1.0|\n",
      "|  1.0|(692,[151,152,153...|[-20.483011833009...|[1.27157209200602...|       1.0|\n",
      "|  0.0|(692,[129,130,131...|[19.8506078990277...|[0.99999999760673...|       0.0|\n",
      "|  1.0|(692,[158,159,160...|[-20.337256674833...|[1.47109814695578...|       1.0|\n",
      "|  1.0|(692,[99,100,101,...|[-19.595579753418...|[3.08850168102648...|       1.0|\n",
      "|  0.0|(692,[154,155,156...|[19.2708803215613...|[0.99999999572670...|       0.0|\n",
      "|  0.0|(692,[127,128,129...|[23.6202328360423...|[0.99999999994480...|       0.0|\n",
      "|  1.0|(692,[154,155,156...|[-24.385235147661...|[2.56818872776502...|       1.0|\n",
      "|  0.0|(692,[153,154,155...|[26.3082522490179...|[0.99999999999624...|       0.0|\n",
      "|  0.0|(692,[151,152,153...|[25.8329060318703...|[0.99999999999396...|       0.0|\n",
      "|  1.0|(692,[129,130,131...|[-19.794609139086...|[2.53110684529571...|       1.0|\n",
      "|  0.0|(692,[154,155,156...|[21.0260440948067...|[0.99999999926123...|       0.0|\n",
      "|  1.0|(692,[150,151,152...|[-22.764979942873...|[1.29806018790937...|       1.0|\n",
      "|  0.0|(692,[124,125,126...|[21.5049307193955...|[0.99999999954235...|       0.0|\n",
      "|  0.0|(692,[152,153,154...|[31.9927184226421...|[0.99999999999998...|       0.0|\n",
      "|  1.0|(692,[97,98,99,12...|[-20.521067180414...|[1.22409115616507...|       1.0|\n",
      "|  1.0|(692,[124,125,126...|[-22.245377742755...|[2.18250475400325...|       1.0|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_summary.predictions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = my_data.randomSplit([0.7,.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lgr = LogisticRegression()\n",
    "lgr_model = lgr.fit(train_data)\n",
    "log_summary = lgr_model.summary\n",
    "log_summary.predictions.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|label|            features|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|  0.0|(692,[100,101,102...|[10.6043582154818...|[0.99997519295872...|       0.0|\n",
      "|  0.0|(692,[121,122,123...|[23.7466076879632...|[0.99999999995136...|       0.0|\n",
      "|  0.0|(692,[123,124,125...|[33.0453709442870...|[0.99999999999999...|       0.0|\n",
      "|  0.0|(692,[126,127,128...|[21.5867548664121...|[0.99999999957831...|       0.0|\n",
      "|  0.0|(692,[126,127,128...|[25.8996372486567...|[0.99999999999435...|       0.0|\n",
      "|  0.0|(692,[126,127,128...|[32.8022375408048...|[0.99999999999999...|       0.0|\n",
      "|  0.0|(692,[127,128,129...|[22.9373802966879...|[0.99999999989074...|       0.0|\n",
      "|  0.0|(692,[150,151,152...|[17.7584051400711...|[0.99999998060799...|       0.0|\n",
      "|  0.0|(692,[151,152,153...|[28.8133134682893...|[0.99999999999969...|       0.0|\n",
      "|  0.0|(692,[152,153,154...|[13.3147660602334...|[0.99999835005352...|       0.0|\n",
      "|  0.0|(692,[153,154,155...|[28.6003013648761...|[0.99999999999962...|       0.0|\n",
      "|  0.0|(692,[154,155,156...|[16.7053725059135...|[0.99999994441611...|       0.0|\n",
      "|  0.0|(692,[155,156,180...|[34.638690260088,...|[0.99999999999999...|       0.0|\n",
      "|  1.0|(692,[99,100,101,...|[-6.0073927763496...|[0.00245445570966...|       1.0|\n",
      "|  1.0|(692,[123,124,125...|[-23.367783538378...|[7.10396077746938...|       1.0|\n",
      "|  1.0|(692,[123,124,125...|[-16.188569176045...|[9.31952325704731...|       1.0|\n",
      "|  1.0|(692,[124,125,126...|[-24.358905685075...|[2.63670580753824...|       1.0|\n",
      "|  1.0|(692,[124,125,126...|[-21.460063260023...|[4.78644358680728...|       1.0|\n",
      "|  1.0|(692,[124,125,126...|[-28.826030233470...|[3.02701194278504...|       1.0|\n",
      "|  1.0|(692,[125,126,127...|[-18.975659651741...|[5.74084366481385...|       1.0|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction = lgr_model.evaluate(test_data)\n",
    "prediction.predictions.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import (BinaryClassificationEvaluator, \n",
    "                                MulticlassClassificationEvaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_eval = BinaryClassificationEvaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_final_roc = my_eval.evaluate(prediction.predictions)\n",
    "my_final_roc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " this mean that the erea on the curve Roc is 1 what is a good sigh that we succed to predict all truly the binary class "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Realistic Exercice Titanic "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attempt to predict what passenger survived the titanic crush based solely on passenger's\n",
    "features (age, cabin, children,...)\n",
    "- deal with missing value , .... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"logReg_test\").getOrCreate()\n",
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PassengerId: integer (nullable = true)\n",
      " |-- Survived: integer (nullable = true)\n",
      " |-- Pclass: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- SibSp: integer (nullable = true)\n",
      " |-- Parch: integer (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Cabin: string (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "my_data = spark.read.csv(\"../../Data/titanic.csv\",inferSchema=True, header= True)\n",
    "my_data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select only the features we need \n",
    "we can use after that feature angering for age for example, transform categorical feaeture to numeric or binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PassengerId',\n",
       " 'Survived',\n",
       " 'Pclass',\n",
       " 'Name',\n",
       " 'Sex',\n",
       " 'Age',\n",
       " 'SibSp',\n",
       " 'Parch',\n",
       " 'Ticket',\n",
       " 'Fare',\n",
       " 'Cabin',\n",
       " 'Embarked']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_cols = my_data.select([\"Survived\",\"Pclass\",\"Sex\",\"Age\",\"SibSp\",\"Parch\",\"Fare\",\"Embarked\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with the missing data \n",
    "to go so fast just here we will delete the missing data but there a lot of solution for that (imputation of missing value ) and drop them directly are the most easiest and worst thing to do "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_final_data = my_cols.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepare our data to fit the Logistic Regression Algorithm\n",
    "\n",
    "# one hot Encoder to transform categorical feature to binary  feature\n",
    "# StringIndexer to to transform categorical feature to numeric \n",
    "from pyspark.ml.feature import (VectorAssembler , VectorIndexer, OneHotEncoder, StringIndexer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_indexer = StringIndexer(inputCol=\"Sex\", outputCol=\"SexIndex\")\n",
    "gender_encoder = OneHotEncoder(inputCol=\"SexIndex\", outputCol=\"SexVec\")\n",
    "\n",
    "embark_indexer = StringIndexer(inputCol=\"Embarked\", outputCol=\"EmbarkedIndex\")\n",
    "embark_encoder = OneHotEncoder(inputCol=\"EmbarkedIndex\", outputCol=\"EmbarkedVec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "assember = VectorAssembler(inputCols=[\"Pclass\",\"SexVec\",\"EmbarkedVec\",\n",
    "                                    \"Age\",\"SibSp\",\"Parch\",\"Fare\"], outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_titanic = LogisticRegression(featuresCol=\"features\", labelCol=\"Survived\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages = [gender_indexer, embark_indexer,gender_encoder,embark_encoder,\n",
    "                             assember,log_reg_titanic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = my_final_data.randomSplit([0.7,.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_model = pipeline.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = fit_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_eval = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\", labelCol=\"Survived\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+\n",
      "|Survived|prediction|\n",
      "+--------+----------+\n",
      "|       0|       1.0|\n",
      "|       0|       1.0|\n",
      "|       0|       1.0|\n",
      "|       0|       0.0|\n",
      "|       0|       0.0|\n",
      "|       0|       0.0|\n",
      "|       0|       0.0|\n",
      "|       0|       0.0|\n",
      "|       0|       0.0|\n",
      "|       0|       0.0|\n",
      "|       0|       0.0|\n",
      "|       0|       0.0|\n",
      "|       0|       0.0|\n",
      "|       0|       0.0|\n",
      "|       0|       0.0|\n",
      "|       0|       1.0|\n",
      "|       0|       1.0|\n",
      "|       0|       1.0|\n",
      "|       0|       0.0|\n",
      "|       0|       0.0|\n",
      "+--------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.select(\"Survived\",\"prediction\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.78586464992983"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AUC = my_eval.evaluate(result)\n",
    "AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
